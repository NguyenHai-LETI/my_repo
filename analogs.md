## Обзор предметной области
### Принцип отбора аналогов
ДДля создания метода выбора признаков необходимо провести обзор существующих алгоритмов выбора признаков. Для поиска алгоритмов выявления признаков использовался следующий запрос: ‘traditional feature selection algorithms’
### Аналоги
### SelectKBest feature selection method [1]
SelectKBest выбирает предварительно заданное количество k наиболее эффективных функций на основе указанной функции оценки. Функция оценки может быть любым статистическим тестом, который присваивает оценку каждой функции, такой как хи-квадрат[2], взаимная информация или F-значение ANOVA [3]. Затем SelectKBest выбирает k объектов с наивысшими оценками. Этот метод прост и эффективен и обычно используется в случаях, когда количество признаков велико, и цель состоит в том, чтобы уменьшить пространство признаков, сохранив при этом наиболее информативные признаки. Недостатком алгоритма является то, что выбор оценочной функции является субъективным, и результаты могут быть чувствительны к этому выбору.
### Recursive Feature Elimination (RFE) [4,5]
Алгоритм RFE работает путем рекурсивного удаления наименее важных признаков из набора данных. Алгоритм начинается с полного набора признаков и обучает модель на этом наборе. Значения важности признаков вычисляются на основе коэффициентов модели. Затем удаляется наименее важный признак и процесс повторяется, пока не будет достигнуто желаемое количество признаков. Основными преимуществами алгоритма RFE является его простота, которая делает его легко реализуемым и понятным. Кроме того, показано, что он эффективен при уменьшении количества признаков в наборе данных, что повышает интерпретируемость модели и ее обобщающую способность. Однако есть также и недостатки. Один из главных недостатков - зависимость от выбора используемой модели, что может быть трудно определить на практике.
### Lasso Regression (Least Absolute Shrinkage and Selection Operator) [4]
Lasso Regression - линейной регрессии, который добавляет термин регуляризации в функцию потерь, чтобы стимулировать модель иметь разреженные коэффициенты. Другими словами, Lasso регрессия пытается уменьшить коэффициенты менее важных признаков до нуля, эффективно уменьшая количество используемых в модели признаков. Это делает Lasso Regression полезным инструментом для выбора признаков, а также для предотвращения
переобучения, уменьшая сложность модели. Одним из его преимуществ является способность выполнять выбор признаков, уменьшая коэффициенты менее важных признаков до нуля, эффективно исключая их из модели. Это может улучшить интерпретируемость модели и предотвратить переобучение. Более того, Lasso Regression способен работать с большим количеством признаков, что делает его подходящим для высокомерных наборов данных. Однако может давать смещенные оценки, когда в наборе данных есть сильно коррелированные признаки.
### Ridge Regression [5]
Ridge Regression - линейной регрессии, который добавляет член регуляризации в функцию стоимости, чтобы предотвратить переобучение. В отличие от Lasso Regression[4], которая может создавать разреженные модели, зануляя коэффициенты менее важных признаков, Ridge Regression сужает коэффициенты к нулю, но никогда не устанавливает их равными нулю. Это делает его подходящей для случаев, когда важно сохранять все признаки в модели. Преимущество Ridge Regression является то, что может лучше обрабатывать коллинеарность (высокая корреляция между признаками) по сравнению с обычной линейной регрессией. Однако одним из егог недостатков является то, что может быть сложно выбрать подходящее значение для константы регуляризации, поскольку большее значение приведет к большему уменьшению коэффициентов, а меньшее значение приведет к меньшему уменьшению. Кроме того, гребенчатая регрессия может работать плохо, если в наборе данных имеется большое количество нерелевантных функций.
### Tree-based on feature selection (Random forest) [6]
Random forest — популярный алгоритм машинного обучения на основе дерева, который можно использовать для выбора признаков. Построены несколько деревьев решений с использованием случайных подмножеств функций, а затем агрегирования важности функций, полученных из каждого дерева. Важности признаков может быть использована для ранжирования признаков, а верхние-k признаки могут быть выбраны для использования в модели.
## Критерии сравнения
Были выбраны следующие основные параметры сравнения:
1. Точность: способность выбрать наиболее релевантные признаки, которые вносят вклад в задачу прогнозирования. Это может быть измерено с помощью оценки производительности модели, построенной с использованием выбранных признаков, по сравнению с производительностью модели, построенной с использованием всех признаков.
2. Временная сложность: относится к времени, необходимому алгоритму для завершения
процесса выбора признаков. Это важный фактор, который следует учитывать при работе
с большими наборами данных, так как алгоритмы выбора признаков могут быть
расчетно трудоемкими и занимать много времени.
3. Масштабируемость: способность алгоритма обрабатывать большие наборы данных с
высоким количеством признаков. Это важно, поскольку количество признаков в наборе
данных может существенно влиять на производительность алгоритма и его способность
завершить процесс выбора признаков в приемлемые сроки.
4. Устойчивость: способность алгоритма производить стабильные и согласованные
результаты на различных наборах данных и задачах. Это важно, поскольку алгоритмы
выбора признаков иногда могут производить различные результаты для различных
наборов данных, даже когда наборы данных похожи. Устойчивый алгоритм будет
производить согласованные результаты на широком диапазоне наборов данных.
5. Интерпретируемость: интерпретируемость относится к легкости, с которой результаты
алгоритма можно понять и интерпретировать пользователем. Это важно, поскольку
подходящий алгоритм выбора признаков не только должен производить точные
результаты, но также должен предоставлять ясное и интерпретируемое объяснение
того, как выбраны функции и почему они важны для задачи предсказания.
В табл.1 представлено сравнение алгоритмов выбора признаков.

**Таблица 1 - Сравнительный анализ алгоритмов по количественным критериям**
| Алгоритм выбора признаков       | Точность   | Временная сложность      | Интерпретируемость    | Масштабируемость   | Устойчивость    |
| --------------------------------| -----------| ------------------------ | ----------------------| -------------------| ----------------|
| SelecKBest                      | Хорошо     | Низко                    | Высоко                | Высоко             | Умеренно        | 
| RFE                             | Хорошо     | Высоко                   | Высоко                | Высоко             | Высоко          | 
| Lasso Regression                | Хорошо     | Умеренно                 | Высоко                | Высоко             | Высоко          | 
| Ridge Regression                | Хорошо     | Высоко                   | Высоко                | Высоко             | Высоко          |
| Random Forest                   | Отлично    | Высоко                   | Низко                 | Высоко             | Отлично         |

## Выводы по итогам сравнения
SelectKBest прост и удобен в реализации, в то время как другие, такие как Random Forest, более
сложны, но предлагают потенциал для более высокой производительности. Методы
регуляризации, такие как Lasso и Bridge regression, можно использовать для предотвращения
переобучения и улучшения интерпретируемости модели. Рекурсивное устранение признаков
(RFE) может быть эффективным для уменьшения количества признаков в наборе данных, но
его производительность зависит от выбора используемой модели. Лучший алгоритм выбора
признаков для конкретной задачи будет зависеть от множества факторов, включая размер и сложность набора данных, желаемый результат и доступные вычислительные ресурсы.